{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Konane.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA 2 (Adversial search: Game)\n",
    "In this project, we try to develop an intelligent agent for playing a board game. <br>\n",
    "This game is deterministix and zero sum. So for solving this problem, we use minimax algorithm. <br>\n",
    "Depth of state space is very large and because of that we can't run this algorithm till leaves of tree. We should do a depth-limited search and use evaluation function for last depth nodes. This function is some how utility function that tells how good a state is. This function make diffrences in this types of agents. <br>\n",
    "For deeper searchs, we should prune some branches. For this purpose, we use alpha-beta pruning algorithm. <br>\n",
    "We discuss about diffrent parts in below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax player\n",
    "We should define a new player that is our agent. It inherits from __Game__ and __Player__ class and extends their functionallities. This player class has five main funcitons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize(self, side, depth)\n",
    "It's overriden method of player class. It sets main features of player instance. \n",
    "(__name__, \n",
    "__side__: Player's color,\n",
    "__depth__: _Show maximum depth that minimax explores it._\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(self, side, depth):\n",
    "    self.name = \"Minimax\"\n",
    "    self.side = side\n",
    "    self.depth = depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getMove(self, board)\n",
    "It's another overriden method of player class that returns player's next move. Game class deal with function to handle player movements. It takes current board configuration and returns player's next move. <br>\n",
    "In this function we create level 1 nodes of minimax tree and behave like a maximizer node because we want to maximize our point. We discuss about minimax in details in next section. It do alpha-beta pruning too. <br> \n",
    "This function aggregate best possible move that get from belower nodes in minimax tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMove(self, board):\n",
    "    best_move_score = float(-inf)\n",
    "    initial_alpha = float(-inf)\n",
    "    initial_beta = float(inf)\n",
    "    moves = self.generateMoves(board, self.side)\n",
    "    if not moves:\n",
    "        return []\n",
    "    for move in moves:\n",
    "        next_board = self.nextBoard(board, self.side, move)\n",
    "        next_board_score = self.minValue(next_board, initial_alpha, initial_beta, 1)\n",
    "        if next_board_score > best_move_score:\n",
    "            best_move = move\n",
    "            best_move_score = next_board_score\n",
    "        if best_move_score > initial_beta:\n",
    "            return best_move_score\n",
    "        initial_alpha = max(initial_alpha, best_move_score)\n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax algorithm backbones\n",
    "### minValue(self, board, alpha, beta, depth)   &   maxValue(self, board, alpha, beta, depth)\n",
    "This two functions implement minimax algorithm main functionallity. In a zero sum game, our opponent try to decrease our point and we try to increase our point. If we have tree that models our move and board states, we can iterate on it to find win scenario and select our moves. Each terminal state has a point that define the point that we get when we reach to this tate at the end of the game. Each terminal state tell it's point to it's parrent. Middle nodes are minimizer nodes or maximizer nodes. In one turn (level), we try to achieve best score and in next our opponent try to increase it's score and because game is zero sum, it decrease our score. So we are maximizer nodes and our opponent or children in minimax tree are minimizer nodes. In maximizer nodes, we calculate maximum values of our children. Minimizer nodes calculate minimum of it's children. <br>\n",
    "In the first part of this function, we see depth limiting. We should make desicion in reasonable time. Because of resource limits, we should do depth limited search. <br>\n",
    "In the last part we can see implemenation of alpha beta prunning. This algorithm prune non promising branches and block them to save time to explore deeper. <br>\n",
    "In this part of algorithm we pass alpha and beta down the tree.\n",
    "<br>\n",
    "α: MAX’s best option on path to root\n",
    "<br>\n",
    "β: MIN’s best option on path to root\n",
    "<br>\n",
    "In minimizer node, if we find a solution that is lower from alpha we don't go to it's n\n",
    "branch and prune it because we know that it's bas way and won't returns a good result for us.\n",
    "It means that this branch and node is not a way that chosen by minimax algorithm. <br>\n",
    "__So alpha beta search doesn't choose diffrent moves from minimax algorithm.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def maxValue(self, board, alpha, beta, depth):\n",
    "        if depth == self.depth:\n",
    "            return self.evaluate(board)\n",
    "        maximum = float(-inf)\n",
    "        moves = self.generateMoves(board, self.side)\n",
    "        if not moves:\n",
    "            return self.evaluate(board)\n",
    "        for move in moves:\n",
    "            next_board = self.nextBoard(board, self.side, move)\n",
    "            maximum = max(maximum, self.minValue(next_board, alpha, beta, depth+1))\n",
    "            if maximum > beta:\n",
    "                return maximum\n",
    "            alpha = max(alpha, maximum)\n",
    "        return maximum\n",
    "\n",
    "\n",
    "    def minValue(self, board, alpha, beta, depth):\n",
    "        if depth == self.depth:\n",
    "            return self.evaluate(board)\n",
    "        minimum = float(inf)\n",
    "        moves = self.generateMoves(board, self.opponent(self.side))\n",
    "        if not moves:\n",
    "            return self.evaluate(board)\n",
    "        for move in moves:\n",
    "            next_board = self.nextBoard(board, self.opponent(self.side), move)\n",
    "            minimum = min(minimum, self.maxValue(next_board, alpha, beta, depth+1))\n",
    "            if minimum < alpha:\n",
    "                return minimum\n",
    "            beta = min(beta, minimum)\n",
    "        return minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Time comparision between alpha-beta VS. minimax search\n",
    "We can see each move time average for each of this algorithm is below table. <br>\n",
    "Alpha beta prunning prunes state space and save a lot of time as you see in below table. <br>\n",
    "By this table and 5 second time limit we can set minimax maximum depth to 3(almost 4) and for alpha beta version set it to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/comparision.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate(self, board)\n",
    "This function evaluates each state utillity(How much is good for win). We use depth-limited search, so our algorithm ends up in a non terminal state. So we can't use main utillity and we shoud use an estimation for it. This estimation has been done by __evaluate__ funciton. <br>\n",
    "Game is over when one player can't move. So i think to two criterias: <br>\n",
    "1. How many moves that we can make and how many our opponent can make\n",
    "    + It's a good measure somehow because how much is lower we have more risk to lose the game.\n",
    "    + I try diffrence, weighted diffrence and weighted ratio but agent with simple ratio wins more games against this agents. So i choose ratio of this two parameter.\n",
    "2. How many movable pices i have versus how many movable pieces my opponent have\n",
    "    + It's a good measure somehow because how much is upper, we have more chance to win the game.\n",
    "    + I try diffrence, weighted diffrence and weighted ratio but agent with simple ratio wins more games against this agents. So i choose ratio of this two parameter.\n",
    "I compete this two types of agents against each other and see that in some situations first wins and in other situations second one wins. So i decide to combine this two parameter with some coefficient to get better evaluation for states.\n",
    "I see this pattern that in board sizes lower than 5, minimax wins my agent. So i decide to make board size a parameter in my evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evaluate(self, board):\n",
    "        my_moves = self.generateMoves(board, self.side)\n",
    "        my_movable_pieces = {(move[0], move[1]) for move in my_moves}\n",
    "        if not my_moves:\n",
    "            return -1000 # Loosing point\n",
    "        opponent_moves = self.generateMoves(board, self.opponent(self.side))\n",
    "        opp_movable_pieces = {(move[0], move[1]) for move in opponent_moves}\n",
    "        if not opponent_moves:\n",
    "            return 1000 # Winning point\n",
    "        first_eval = len(my_moves) / len(opponent_moves)\n",
    "        second_eval = len(my_movable_pieces)/len(opp_movable_pieces)\n",
    "        return first_eval + 2 * second_eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
